{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZRwNvPDi7_7"
   },
   "source": [
    "# Text Generator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwxEMqIljMMn"
   },
   "source": [
    "This notebook contains a deep learning algorithm used to generate different texts based on a seed (sentence). Below, the first steps taken are presented: importing the required libraries and downloading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "aWIY5GBu2B05"
   },
   "outputs": [],
   "source": [
    "#Importing relevant libraries \n",
    "from essential_generators import DocumentGenerator\n",
    "from essential_generators import MarkovWordGenerator\n",
    "from essential_generators import MarkovTextGenerator\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from string import punctuation\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import requests\n",
    "import pickle\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rcP5y9Y12B09",
    "outputId": "96b35929-39db-4a46-e1cf-4b1469b0cd10"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/ESADE/wonderland.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1c0ec22bcb2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Downloading dataset ('Wonderland' book) and saving it in a folder called 'data'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http://www.gutenberg.org/cache/epub/11/pg11.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/ESADE/wonderland.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/ESADE/wonderland.txt'"
     ]
    }
   ],
   "source": [
    "#Downloading dataset ('Wonderland' book) and saving it in a folder called 'data'.\n",
    "content = requests.get(\"http://www.gutenberg.org/cache/epub/11/pg11.txt\").text\n",
    "open(\"/ESADE/wonderland.txt\", \"w\", encoding=\"utf-8\").write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cs5JBIuuk9O_"
   },
   "source": [
    "## Transformations of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUQNaTBSlCRq"
   },
   "source": [
    "In this section of the code, several steps are taken to transform and extract the data required to train the deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ChG4Ukh2B0_"
   },
   "outputs": [],
   "source": [
    "#Declaration of relevant variables to train the model\n",
    "sequence_length = 100\n",
    "BATCH_SIZE = 300\n",
    "EPOCHS = 30 \n",
    "\n",
    "#Saving the dataset file path (data folder)\n",
    "FILE_PATH = \"/content/drive/MyDrive/ESADE/data/wonderland.txt\"\n",
    "BASENAME = os.path.basename(FILE_PATH)\n",
    "\n",
    "#Reading the dataset\n",
    "text = open(FILE_PATH, encoding=\"utf-8\").read()[:-50000]\n",
    "\n",
    "#Removing capital letters and punctuation\n",
    "text = text.lower() \n",
    "text = text.translate(str.maketrans(\"\", \"\", punctuation))\n",
    "\n",
    "vocab = ''.join(sorted(set(text)))  #sorted vocabulary of the text (dataset)\n",
    "n_chars = len(text)                 #number of characters of the text\n",
    "n_unique_chars = len(vocab)         #number of unique characters of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ky0EeoYKlkSy"
   },
   "source": [
    "Below, two dictionaries are being created and saved to generate texts based on them. One of the dictionaries has the characters of the text converted to integers and the other one the integers converted to characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "g0CMabeJ2B1B"
   },
   "outputs": [],
   "source": [
    "#Creating and saving dictionary that contains characters converted to integers\n",
    "char2int = {c: i for i, c in enumerate(vocab)}\n",
    "pickle.dump(char2int, open(f\"{BASENAME}-char2int.pickle\", \"wb\"))\n",
    "\n",
    "#Creating and saving dictionary that contains integers converted to characters\n",
    "int2char = {i: c for i, c in enumerate(vocab)}\n",
    "pickle.dump(int2char, open(f\"{BASENAME}-int2char.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OR2TweCynq-7"
   },
   "source": [
    "Further steps are taken below to create sequences of texts (sentences or paragraphs). For this, 2 main functions are generated: split_sample( ) and one_hot_samples( )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "iMl4cS4B2B1C"
   },
   "outputs": [],
   "source": [
    "#Converting all the text into integers\n",
    "encoded_text = np.array([char2int[c] for c in text])\n",
    "\n",
    "#Building tf.data.Dataset object from the encoded text\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
    "\n",
    "#Building sequences by batching\n",
    "sequences = char_dataset.batch(2*sequence_length + 1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "SU8IndUU2B1E"
   },
   "outputs": [],
   "source": [
    "def split_sample(sample):\n",
    "  '''\n",
    "  Function to split the text in tuple of inputs and targets.\n",
    "\n",
    "  Parameters:\n",
    "  sample: Sequences of characters from dataset.\n",
    "\n",
    "  Output: \n",
    "  ds: Tuples of inputs (previous characters) and targets (next characters).\n",
    "  '''\n",
    "  ds = tf.data.Dataset.from_tensors((sample[:sequence_length], sample[sequence_length]))\n",
    "  for i in range(1, (len(sample)-1) // 2):\n",
    "    input_ = sample[i: i+sequence_length]\n",
    "    target = sample[i+sequence_length]\n",
    "    other_ds = tf.data.Dataset.from_tensors((input_, target))\n",
    "    ds = ds.concatenate(other_ds)\n",
    "    return ds\n",
    "\n",
    "def one_hot_samples(input_, target):\n",
    "    '''\n",
    "  Function to one hot encode the tuples of inputs and targets.\n",
    "\n",
    "  Parameters:\n",
    "  input_: Previous characters.\n",
    "  target: Next characters expected based on input_.\n",
    "\n",
    "  Output: \n",
    "  Tuples of inputs (previous characters) and targets (next characters) one hot encoded.\n",
    "  '''\n",
    "    return tf.one_hot(input_, n_unique_chars), tf.one_hot(target, n_unique_chars)\n",
    "\n",
    "#Preparing the inputs and targets\n",
    "dataset = sequences.flat_map(split_sample)\n",
    "dataset = dataset.map(one_hot_samples)\n",
    "\n",
    "#Repeat, shuffle and batch the dataset\n",
    "ds = dataset.repeat().shuffle(1024).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkEOV5lusdt6"
   },
   "source": [
    "## Building, training and saving the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26P0eMVnsk84"
   },
   "source": [
    "In this section, a deep learning model with a simple architecture is created, trained and saved to be later used to predict sentences based on different seeds (other phrases, texts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f9YMIGFQ2B1H",
    "outputId": "0c411bd9-07be-44ad-96e9-ded196e764fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 100, 256)          301056    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 100, 256)          0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 256)               525312    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 37)                9509      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 835,877\n",
      "Trainable params: 835,877\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Building a sequential deep learning model (using neural networks)\n",
    "model = Sequential([\n",
    "    LSTM(256, input_shape=(sequence_length, n_unique_chars), return_sequences=True), #Input layer of the model with shape (none, 100, 256)\n",
    "    Dropout(0.3),                                                                    #Dropout to reduce overfitting\n",
    "    LSTM(256),                                                                       #Hidden layer with same shape as input layer.\n",
    "    Dense(n_unique_chars, activation=\"softmax\"),                                     #Final dense layer with activation function softmax and shape (none,37)\n",
    "])\n",
    "\n",
    "#Defining the path where the model needs to be saved (results folder)\n",
    "model_weights_path = f\"results/{BASENAME}-{sequence_length}.h5\"\n",
    "\n",
    "#Printing a summary of the architecture of the model and compiling the metric, loss and optimizer of the model.\n",
    "model.summary()\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TK3hZMOs2B1H",
    "outputId": "2e552614-8084-4362-e339-5ffbf1285c83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "357/357 [==============================] - 31s 78ms/step - loss: 2.5933 - accuracy: 0.2602\n",
      "Epoch 2/30\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 2.0948 - accuracy: 0.3857\n",
      "Epoch 3/30\n",
      "357/357 [==============================] - 28s 79ms/step - loss: 1.8687 - accuracy: 0.4479\n",
      "Epoch 4/30\n",
      "357/357 [==============================] - 28s 79ms/step - loss: 1.7187 - accuracy: 0.4874\n",
      "Epoch 5/30\n",
      "357/357 [==============================] - 29s 82ms/step - loss: 1.6001 - accuracy: 0.5177\n",
      "Epoch 6/30\n",
      "357/357 [==============================] - 28s 79ms/step - loss: 1.5015 - accuracy: 0.5431\n",
      "Epoch 7/30\n",
      "357/357 [==============================] - 28s 79ms/step - loss: 1.4135 - accuracy: 0.5698\n",
      "Epoch 8/30\n",
      "357/357 [==============================] - 28s 79ms/step - loss: 1.3275 - accuracy: 0.5932\n",
      "Epoch 9/30\n",
      "357/357 [==============================] - 28s 79ms/step - loss: 1.2365 - accuracy: 0.6194\n",
      "Epoch 10/30\n",
      "357/357 [==============================] - 28s 79ms/step - loss: 1.1500 - accuracy: 0.6453\n",
      "Epoch 11/30\n",
      "357/357 [==============================] - 28s 79ms/step - loss: 1.0653 - accuracy: 0.6693\n",
      "Epoch 12/30\n",
      "357/357 [==============================] - 28s 79ms/step - loss: 0.9774 - accuracy: 0.6968\n",
      "Epoch 13/30\n",
      "357/357 [==============================] - 28s 79ms/step - loss: 0.9022 - accuracy: 0.7182\n",
      "Epoch 14/30\n",
      "357/357 [==============================] - 28s 79ms/step - loss: 0.8265 - accuracy: 0.7432\n",
      "Epoch 15/30\n",
      "357/357 [==============================] - 28s 79ms/step - loss: 0.7562 - accuracy: 0.7641\n",
      "Epoch 16/30\n",
      "357/357 [==============================] - 29s 82ms/step - loss: 0.6902 - accuracy: 0.7830\n",
      "Epoch 17/30\n",
      "357/357 [==============================] - 28s 79ms/step - loss: 0.6342 - accuracy: 0.8009\n",
      "Epoch 18/30\n",
      "357/357 [==============================] - 28s 79ms/step - loss: 0.5755 - accuracy: 0.8196\n",
      "Epoch 19/30\n",
      "357/357 [==============================] - 28s 78ms/step - loss: 0.5289 - accuracy: 0.8329\n",
      "Epoch 20/30\n",
      "357/357 [==============================] - 28s 79ms/step - loss: 0.4832 - accuracy: 0.8485\n",
      "Epoch 21/30\n",
      "357/357 [==============================] - 28s 79ms/step - loss: 0.4479 - accuracy: 0.8578\n",
      "Epoch 22/30\n",
      "357/357 [==============================] - 28s 79ms/step - loss: 0.4110 - accuracy: 0.8701\n",
      "Epoch 23/30\n",
      "357/357 [==============================] - 28s 79ms/step - loss: 0.3772 - accuracy: 0.8802\n",
      "Epoch 24/30\n",
      "357/357 [==============================] - 28s 79ms/step - loss: 0.3480 - accuracy: 0.8896\n",
      "Epoch 25/30\n",
      "357/357 [==============================] - 28s 79ms/step - loss: 0.3220 - accuracy: 0.8982\n",
      "Epoch 26/30\n",
      "357/357 [==============================] - 28s 79ms/step - loss: 0.2984 - accuracy: 0.9053\n",
      "Epoch 27/30\n",
      "357/357 [==============================] - 29s 81ms/step - loss: 0.2775 - accuracy: 0.9131\n",
      "Epoch 28/30\n",
      "357/357 [==============================] - 28s 79ms/step - loss: 0.2640 - accuracy: 0.9168\n",
      "Epoch 29/30\n",
      "357/357 [==============================] - 28s 79ms/step - loss: 0.2465 - accuracy: 0.9218\n",
      "Epoch 30/30\n",
      "357/357 [==============================] - 28s 79ms/step - loss: 0.2321 - accuracy: 0.9266\n"
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "model.fit(ds, steps_per_epoch=(len(encoded_text) - sequence_length) // BATCH_SIZE, epochs=EPOCHS)\n",
    "\n",
    "#Saving the model in the defined path\n",
    "model.save(model_weights_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text_generator.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
